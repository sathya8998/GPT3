{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMbVxS3tT1w58i851cwJVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathya8998/GPT3/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHVEnWAeVSEK",
        "outputId": "0929b820-6904-45f4-a82c-859a58941bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-xla in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torch-xla) (1.4.0)\n",
            "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from torch-xla) (0.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from torch-xla) (6.0.1)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.1.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.34.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "pip install torch-xla"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CaZzvxqVknF",
        "outputId": "c34a0780-038a-401d-fc4a-9ba4f21361ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.10/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import langid\n",
        "import logging  # Added import for logging\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 64\n",
        "BLOCK_SIZE = 256\n",
        "MAX_ITERS = 5000\n",
        "EVAL_INTERVAL = 500\n",
        "LEARNING_RATE = 3e-4\n",
        "DEVICE = xm.xla_device()\n",
        "EVAL_ITERS = 200\n",
        "N_EMBD = 384\n",
        "N_HEAD = 6\n",
        "N_LAYER = 6\n",
        "DROPOUT = 0.2\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Reading text data\n",
        "with open('/content/Data.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text into sentences or smaller chunks (adjust as needed)\n",
        "sentences = [sentence.strip() for sentence in text.split('.')]\n",
        "all_tokens = [token for sentence in sentences for token in sentence.split()]\n",
        "english_german_vocab = set(all_tokens)\n",
        "VOCAB_SIZE = len(english_german_vocab)\n",
        "\n",
        "# Update BLOCK_SIZE based on a fixed value\n",
        "BLOCK_SIZE = 256\n",
        "print(\"Updated BLOCK_SIZE:\", BLOCK_SIZE)\n",
        "\n",
        "def encode_multilingual(s, vocab_size, lang, max_len=BLOCK_SIZE):\n",
        "    if lang == 'de':\n",
        "        return [(ord(c) % vocab_size) + vocab_size for c in s[:max_len] if (ord(c) % vocab_size) + vocab_size < vocab_size] + [0] * (max_len - len(s))\n",
        "    else:\n",
        "        return [ord(c) % vocab_size for c in s[:max_len] if ord(c) % vocab_size < vocab_size] + [0] * (max_len - len(s))\n",
        "\n",
        "# Update the vocab size to include characters from both languages\n",
        "VOCAB_SIZE = len(english_german_vocab)\n",
        "\n",
        "# Convert text to tokens and pad sequences\n",
        "tokenized_text = [encode_multilingual(token, VOCAB_SIZE, 'en') for token in all_tokens]\n",
        "max_len = BLOCK_SIZE\n",
        "padded_text = [seq + [0] * (max_len - len(seq)) for seq in tokenized_text]\n",
        "\n",
        "# Data loading\n",
        "data = torch.tensor(padded_text, dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(train_data[:-1], train_data[1:])\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "val_dataset = TensorDataset(val_data[:-1], val_data[1:])\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Function to decode the generated tokens into a string\n",
        "def decode_multilingual(tokens):\n",
        "    return \"\".join(chr(token) for token in tokens)\n",
        "\n",
        "# Modified GPT Language Model for chat-like responses\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head=8):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = nn.MultiheadAttention(n_embd, n_head)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(DROPOUT),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x), x, x)[0]\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class ChatGPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
        "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
        "        self.blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])\n",
        "        self.ln_f = nn.LayerNorm(N_EMBD)\n",
        "        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "    def respond_to_question(self, question):\n",
        "        # Detect language of the question\n",
        "        lang, _ = langid.classify(question)\n",
        "\n",
        "        # Tokenize the user's question and convert it to model input\n",
        "        if lang == 'de':\n",
        "            user_input = torch.tensor(encode_multilingual_german(question, VOCAB_SIZE, lang), dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "        else:\n",
        "            user_input = torch.tensor(encode_multilingual_english(question, VOCAB_SIZE, lang), dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # Generate response based on the user's input\n",
        "        response = self.generate(user_input, max_new_tokens=50)\n",
        "\n",
        "        # Decode and return the response\n",
        "        return decode_multilingual(response[0].tolist())\n",
        "\n",
        "# Create model and move to device\n",
        "model = ChatGPTLanguageModel(VOCAB_SIZE)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Wrapping DataLoader for TPU\n",
        "train_loader = pl.MpDeviceLoader(train_loader, DEVICE)\n",
        "\n",
        "# Wrap model for parallel training\n",
        "model_parallel = xmp.MpModelWrapper(model)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(filename='training.log', level=logging.INFO)\n",
        "\n",
        "def estimate_loss(model, dataloader, device):\n",
        "    if isinstance(model, xmp.MpModelWrapper):\n",
        "        # Retrieve the underlying model from the model parallel wrapper\n",
        "        model = model._model\n",
        "\n",
        "    model.eval()  # Evaluate the underlying model\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            # Detect language of the batch\n",
        "            lang, _ = langid.classify(decode_multilingual(xb.flatten()))\n",
        "\n",
        "            # Ensure indices are within the vocabulary size\n",
        "            if (xb >= model.token_embedding_table.weight.shape[0]).any() or (\n",
        "                yb >= model.token_embedding_table.weight.shape[0]\n",
        "            ).any():\n",
        "                logging.warning(\"Index out of range in embedding. Skipping batch.\")\n",
        "                continue\n",
        "\n",
        "            logits, loss = model(xb, yb)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "    return total_loss / total_batches\n",
        "\n",
        "for iteration in range(MAX_ITERS):\n",
        "    try:\n",
        "        # Every once in a while evaluate the loss on train and val sets\n",
        "        if iteration % EVAL_INTERVAL == 0 or iteration == MAX_ITERS - 1:\n",
        "            losses = estimate_loss(model_parallel, train_loader, DEVICE)\n",
        "            logging.info(f\"step {iteration}: train loss {losses:.4f}\")\n",
        "\n",
        "        # Sample a batch of data\n",
        "        xb, yb = next(iter(train_loader))\n",
        "\n",
        "        # Detect language of the batch\n",
        "        lang, _ = langid.classify(decode_multilingual(xb.flatten()))\n",
        "\n",
        "        # Ensure indices are within the vocabulary size\n",
        "        xb = encode_multilingual(xb.flatten(), VOCAB_SIZE, lang).view_as(xb)\n",
        "        yb = encode_multilingual(yb.flatten(), VOCAB_SIZE, lang).view_as(yb)\n",
        "\n",
        "        # Evaluate the loss\n",
        "        logits, loss = model_parallel(xb, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Exception at step {iteration}: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# After the loop, print the maximum index in the entire dataset\n",
        "max_index_dataset = torch.max(data).item()\n",
        "print(\"Max index in the entire dataset:\", max_index_dataset)\n",
        "\n",
        "# Print the maximum index in the embeddings\n",
        "max_index_embeddings = model.token_embedding_table.weight.shape[0] - 1\n",
        "print(\"Max index in the embeddings:\", max_index_embeddings)\n",
        "\n",
        "# Example of responding to a user's question\n",
        "user_question_german = \"Wie effizient sind unsere jetzigen und zukünftigen Optimierungsmaßnahmen?\"\n",
        "user_question_english = \"How efficient are our current and future optimization measures?\"\n",
        "\n",
        "# Use the original model for the German question\n",
        "response_original_german = decode_multilingual(model.respond_to_question(user_question_german)[0].tolist())\n",
        "print(\"Original model response (German):\", response_original_german)\n",
        "\n",
        "# Use the model parallel wrapper for the English question\n",
        "response_parallel_english = decode_multilingual(model_parallel.respond_to_question(user_question_english)[0].tolist())\n",
        "print(\"Parallel model response (English):\", response_parallel_english)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "YJA61VEHX83j",
        "outputId": "124fe724-257f-419f-dc98-42379f6c045d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated BLOCK_SIZE: 256\n",
            "13.94182 M parameters\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6e79c302a3c4>\u001b[0m in \u001b[0;36m<cell line: 206>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVAL_INTERVAL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMAX_ITERS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iteration}: train loss {losses:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6e79c302a3c4>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# Detect language of the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlangid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_multilingual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Ensure indices are within the vocabulary size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6e79c302a3c4>\u001b[0m in \u001b[0;36mdecode_multilingual\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Function to decode the generated tokens into a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_multilingual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Modified GPT Language Model for chat-like responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             )\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}